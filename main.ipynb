{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, ResNet50\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from PIL import Image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_path = \"./list_attr_celeba.csv/list_attr_celeba.csv\"\n",
    "eval_path = \"./list_eval_partition.csv/list_eval_partition.csv\"\n",
    "images_path = \"./img_align_celeba/img_align_celeba/\"\n",
    "attr_df = pd.read_csv(attribute_path, index_col= 'image_id')\n",
    "eval_df = pd.read_csv(eval_path, index_col = 'image_id')\n",
    "attr_df.replace(to_replace = -1, value = 0, inplace = True)\n",
    "data_df =  pd.merge(attr_df[\"Male\"], eval_df, left_on = 'image_id', right_on='image_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Gender Distribution\")\n",
    "sns.countplot(y = 'Male', data = attr_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (224, 224)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_artifacts(training_size ,validation_size, testing_size):\n",
    "    \n",
    "    # Training Data\n",
    "    train_df = data_df[data_df[\"partition\"] == 0].drop('partition', axis = 1)\n",
    "    male_df = train_df[train_df['Male'] == 1]\n",
    "    female_df = train_df[train_df['Male'] == 0]\n",
    "\n",
    "    male_samples = male_df.sample(training_size // 2)\n",
    "    female_samples = female_df.sample(training_size // 2)\n",
    "\n",
    "    train_df = pd.concat([male_samples, female_samples])\n",
    "\n",
    "    # Validation Data\n",
    "    valid_df = data_df[data_df[\"partition\"] == 1].drop('partition', axis = 1)\n",
    "    male_df = valid_df[valid_df['Male'] == 1]\n",
    "    female_df = valid_df[valid_df['Male'] == 0]\n",
    "\n",
    "    male_samples = male_df.sample(validation_size // 2)\n",
    "    female_samples = female_df.sample(validation_size // 2)\n",
    "\n",
    "    valid_df = pd.concat([male_samples, female_samples])\n",
    "\n",
    "    # Testing Data\n",
    "    test_df = data_df[data_df[\"partition\"] == 2].drop('partition', axis = 1)\n",
    "    male_df = test_df[test_df['Male'] == 1]\n",
    "    female_df = test_df[test_df['Male'] == 0]\n",
    "\n",
    "    male_samples = male_df.sample(testing_size // 2)\n",
    "    female_samples = female_df.sample(testing_size // 2)\n",
    "\n",
    "    test_df = pd.concat([male_samples, female_samples])\n",
    "    \n",
    "    # Training Data\n",
    "    predict_train_images = []\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "\n",
    "    for index, row in train_df.iterrows():\n",
    "        image_path = os.path.join(images_path + row.name)\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.resize(image, img_size)\n",
    "        image = np.array(image, dtype = np.float32) / 255.0\n",
    "        train_images.append(image)\n",
    "        train_labels.append(row[\"Male\"])\n",
    "        image = np.reshape(image, (1, 224, 224, 3))\n",
    "        predict_train_images.append(image)\n",
    "\n",
    "    train_images = np.array(train_images)\n",
    "    train_labels = np.array(train_labels)\n",
    "\n",
    "    # Validation Data\n",
    "\n",
    "    valid_images = []\n",
    "    valid_labels = []\n",
    "\n",
    "    for index, row in valid_df.iterrows():\n",
    "        image_path = os.path.join(images_path + row.name)\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.resize(image, img_size)\n",
    "        image = np.array(image, dtype = np.float32) / 255.0\n",
    "        valid_images.append(image)\n",
    "        valid_labels.append(row[\"Male\"])\n",
    "\n",
    "    valid_images = np.array(valid_images)\n",
    "    valid_labels = np.array(valid_labels)\n",
    "\n",
    "    # Testing Data\n",
    "\n",
    "    test_images = []\n",
    "    test_labels = []\n",
    "\n",
    "    for index, row in test_df.iterrows():\n",
    "        image_path = os.path.join(images_path + row.name)\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.resize(image, img_size)\n",
    "        image = np.array(image, dtype = np.float32) / 255.0\n",
    "        test_images.append(image)\n",
    "        test_labels.append(row[\"Male\"])\n",
    "\n",
    "    test_images = np.array(test_images)\n",
    "    test_labels = np.array(test_labels)\n",
    "    \n",
    "    return train_images, train_labels, valid_images, valid_labels, test_images, test_labels, predict_train_images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred, mse):\n",
    "    alpha = 1\n",
    "    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    return bce + alpha * mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_algorithm_model(x_model, mse):\n",
    "    for layer in x_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    #Add new classifier layers on top of the ResNet50 base layers\n",
    "    x = Flatten()(x_model.output)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs = x_model.input, outputs = x)\n",
    "    model.compile(loss=lambda y_true, y_pred: custom_loss(y_true, y_pred, mse), optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a model with the modified architecture\n",
    "def create_model(x_model):\n",
    "    for layer in x_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    #Add new classifier layers on top of the ResNet50 base layers\n",
    "    x = Flatten()(x_model.output)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # x = x_model.output\n",
    "    # x = GlobalAveragePooling2D()(x)\n",
    "    # x = Dense(1024, activation='relu')(x)\n",
    "    # predicitions = Dense(1, activation='softmax')(x)\n",
    "        \n",
    "    model = Model(inputs = x_model.input, outputs = x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # model.compile(loss=custom_loss, optimizer='adam', metrics=['accuracy'])\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains the model on the Dataset and prints the accuracy\n",
    "def FineTune(model, train_images, train_labels, val_images, val_labels, test_images, test_labels, batch_size, file_name):\n",
    "    history = model.fit(\n",
    "        train_images,\n",
    "        train_labels,\n",
    "        batch_size = batch_size,\n",
    "        steps_per_epoch = len(train_images) // batch_size,\n",
    "        validation_data = (val_images, val_labels),\n",
    "        epochs = 5,\n",
    "        callbacks=[EarlyStopping(patience=3), ModelCheckpoint('temp.h5', save_best_only=True)]\n",
    "    )\n",
    "    \n",
    "    scores = model.evaluate(test_images, test_labels)\n",
    "    print(\"Test Loss:\", scores[0])\n",
    "    print(\"Test Accuracy:\", scores[1])\n",
    "    \n",
    "    if file_name != \"no\":\n",
    "        model.save_weights(\"models/\" + file_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_confident_samples(model, predict_train_images, train_images):\n",
    "    confident_images = []\n",
    "    confident_labels = []\n",
    "    for index, image in enumerate(predict_train_images):\n",
    "        prediction = model.predict(image)\n",
    "        if prediction < 0.1 or prediction > 0.9:\n",
    "            confident_images.append(train_images[index])\n",
    "            if prediction < 0.1:\n",
    "                confident_labels.append(0)\n",
    "            else:\n",
    "                confident_labels.append(1)\n",
    "    print(len(confident_labels))\n",
    "\n",
    "    confident_images = np.array(confident_images)\n",
    "    confident_labels = np.array(confident_labels)\n",
    "    return confident_images, confident_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consistency Regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pertubations = [\"color-jitter\", \"horizontal-flippping\"]\n",
    "\n",
    "def pertubate(image_path):\n",
    "\n",
    "    img = cv2.imread(image_path)\n",
    "    order = random.randint(0, 1)\n",
    "    if order == 0:\n",
    "        # Define the random color jitter parameters\n",
    "        brightness = np.random.randint(-30, 30)\n",
    "        contrast = np.random.uniform(0.5, 1.5)\n",
    "        saturation = np.random.uniform(0.5, 1.5)\n",
    "        hue = np.random.randint(-10, 10)\n",
    "\n",
    "        # Convert image to HSV color space\n",
    "        img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        # Apply brightness, contrast, and saturation adjustments to the image\n",
    "        img_hsv[:,:,2] = cv2.addWeighted(img_hsv[:,:,2], contrast, 0, brightness, 0)\n",
    "        img_hsv[:,:,1] = cv2.addWeighted(img_hsv[:,:,1], saturation, 0, 0, 0)\n",
    "\n",
    "        # Convert the image back to BGR color space\n",
    "        jittered_img = cv2.cvtColor(img_hsv, cv2.COLOR_HSV2BGR)\n",
    "        \n",
    "        return jittered_img\n",
    "    \n",
    "    if order == 1:\n",
    "        flipped_img = np.fliplr(img)\n",
    "        return flipped_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_generalizable_images(generalizable_size):\n",
    "    # Generalizable Data \n",
    "    generalizable_df = data_df.sample(generalizable_size).drop('partition', axis = 1)\n",
    "    \n",
    "    original_images = []\n",
    "    pertubated_images = []\n",
    "    \n",
    "    for index, row in generalizable_df.iterrows():\n",
    "        try:\n",
    "            image_path = os.path.join(images_path + row.name)\n",
    "            image = cv2.imread(image_path)\n",
    "            image = cv2.resize(image, img_size)\n",
    "            image = np.array(image, dtype = np.float32) / 255.0\n",
    "            image = np.reshape(image, (1, 224, 224, 3))\n",
    "            original_images.append(image)\n",
    "            \n",
    "            pertubated_image = pertubate(image_path)\n",
    "            pertubated_image = cv2.resize(pertubated_image, img_size)\n",
    "            pertubated_image = np.array(pertubated_image, dtype = np.float32) / 255.0\n",
    "            pertubated_image = np.reshape(pertubated_image, (1, 224, 224, 3))\n",
    "            pertubated_images.append(pertubated_image)\n",
    "        except Exception as err:\n",
    "            print(\"Failed at Index\", index, \" Erros is:\", err)\n",
    "        \n",
    "    return original_images, pertubated_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mse(model, generalizable_size):\n",
    "    \n",
    "    original_images, pertubated_images = pick_generalizable_images(generalizable_size)\n",
    "    mean_square_error = 0\n",
    "    \n",
    "    for index, original_image in enumerate(original_images):\n",
    "        pertubated_image = pertubated_images[index]\n",
    "        original_prediction = model.predict(original_image)\n",
    "        pertubated_prediction = model.predict(pertubated_image)\n",
    "        if (original_prediction > 0.5 and pertubated_prediction > 0.5) or (original_prediction < 0.5 and pertubated_prediction < 0.5):\n",
    "            mean_square_error += 0\n",
    "        else:\n",
    "            mean_square_error += 1\n",
    "    mean_square_error = mean_square_error / generalizable_size\n",
    "    return mean_square_error\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_training_samples = [100, 200, 500, 1000]\n",
    "cotrainteach_sample = 200\n",
    "num_iterations_alg = 4\n",
    "total_sample_size = len(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(labelled_training_samples)):    \n",
    "    labelled_data_size = labelled_training_samples[i]\n",
    "    print(\"Training on Labelled Size:\", labelled_data_size)\n",
    "    # Initial Training\n",
    "    training_size = labelled_data_size\n",
    "    validation_size = training_size // 3\n",
    "    testing_size = training_size // 3 \n",
    "    train_images, train_labels, valid_images, valid_labels, test_images, test_labels, pretrained_images = create_data_artifacts(training_size, validation_size, testing_size)\n",
    "    train_images1, train_labels1, valid_images1, valid_labels1, test_images1, test_labels1, pretrained_images1 = create_data_artifacts(training_size, validation_size, testing_size)\n",
    "    \n",
    "    \n",
    "    vgg1 = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    vgg2 = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    \n",
    "    vgg1 = create_model(vgg1)\n",
    "    vgg2 = create_model(vgg2)\n",
    "    \n",
    "    filename1 = str(training_size) + \"_vgg1_fine_tuned.h5\"\n",
    "    filename2 = str(training_size) + \"_vgg2_fine_tuned.h5\"\n",
    "    \n",
    "    # vgg1 = FineTune(vgg1, train_images, train_labels, valid_images, valid_labels, test_images, test_labels, batch_size, filename1)\n",
    "    # vgg2 = FineTune(vgg1, train_images1, train_labels1, valid_images1, valid_labels1, test_images1, test_labels1, batch_size, filename2)\n",
    "    \n",
    "    print(\"Model Weights for Initial Training saved for Labelled size : \", labelled_data_size)\n",
    "    \n",
    "    generalizable_size = total_sample_size - labelled_data_size\n",
    "    \n",
    "    print(\"Calculating Mean Square Error Losses\")\n",
    "    vgg1_mse = calculate_mse(vgg1, generalizable_size)\n",
    "    vgg2_mse = calculate_mse(vgg2, generalizable_size)\n",
    "    \n",
    "    print(\"Consistency Regularization Loss Computed\")\n",
    "    \n",
    "    # Algorithm Training\n",
    "    for j in range(num_iterations_alg):\n",
    "        \n",
    "        print(\"Algorithm Iteration :\", j+1)\n",
    "        \n",
    "        training_size = cotrainteach_sample\n",
    "        validation_size = training_size // 3\n",
    "        testing_size = training_size // 3\n",
    "        \n",
    "        vgg1 = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "        vgg2 = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    \n",
    "        vgg1 = create_algorithm_model(vgg1, vgg1_mse)\n",
    "        vgg2 = create_algorithm_model(vgg2, vgg2_mse)\n",
    "        \n",
    "    \n",
    "        vgg1.load_weights('models/' + filename1)\n",
    "        vgg2.load_weights('models/' + filename2)\n",
    "        \n",
    "        # Unsupervised Part - A \n",
    "        train_images1, train_labels1, valid_images1, valid_labels1, test_images1, test_labels1, predict_train_images1 = create_data_artifacts(training_size, validation_size, testing_size)\n",
    "        train_images2, train_labels2, valid_images2, valid_labels2, test_images2, test_labels2, predict_train_images2 = create_data_artifacts(training_size, validation_size, testing_size)\n",
    "        \n",
    "        print(\"Finding Confident Samples\")\n",
    "        vgg1_confident_images, vgg1_confident_labels = find_confident_samples(vgg1, predict_train_images1, train_images1)\n",
    "        vgg2_confident_images, vgg2_confident_labels = find_confident_samples(vgg2, predict_train_images2, train_images2)\n",
    "        \n",
    "        if j % 2 == 1:\n",
    "        # Co- Training\n",
    "            print(\"Performing Co- Training\")\n",
    "            vgg1 = FineTune(vgg1, vgg1_confident_images, vgg1_confident_labels, valid_images1, valid_labels1, test_images1, test_labels1, batch_size, \"no\")\n",
    "            vgg2 = FineTune(vgg2, vgg2_confident_images, vgg2_confident_labels, valid_images2, valid_labels2, test_images2, test_labels2, batch_size, \"no\")\n",
    "        else:\n",
    "            # Co - Teaching\n",
    "            print(\"Performing Co- Teaching\")\n",
    "            vgg1 = FineTune(vgg1, vgg2_confident_images, vgg2_confident_labels, valid_images2, valid_labels2, test_images2, test_labels2, batch_size, \"no\")\n",
    "            vgg2 = FineTune(vgg2, vgg1_confident_images, vgg1_confident_labels, valid_images1, valid_labels1, test_images1, test_labels1, batch_size, \"no\")\n",
    "    \n",
    "    filename1  = str(training_size) + \"_vgg1_algorithm.h5\"\n",
    "    filename2  = str(training_size) + \"_vgg2_algorithm.h5\"\n",
    "    \n",
    "    vgg1.save_weights(\"models/\" + filename1)\n",
    "    vgg2.save_weights(\"models/\" + filename2)\n",
    "    \n",
    "    print(\"Model Weights for Algorithm saved for Labelled size : \", labelled_data_size)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(\"models/\"):\n",
    "    if file.endswith(\".h5\"):\n",
    "        print(\"Computing Resuts for Model: \", file)\n",
    "        vgg = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "        vgg = create_model(vgg1)\n",
    "        vgg.load_weights(\"models/\" + file)\n",
    "        \n",
    "        test_images, test_labels, _, _, _, _, _ = create_data_artifacts(25000, 2, 2)\n",
    "        scores = vgg.evaluate(test_images, test_labels)\n",
    "        print(\"Test Loss:\", scores[0])\n",
    "        print(\"Test Accuracy:\", scores[1])\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
